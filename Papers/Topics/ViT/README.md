### ViT
|Title|Poster|Resources|Pic|
|------|------|------|------|
| üèÜ‚≠ê[MLP Can Be A Good Transformer Learner ](https://openaccess.thecvf.com/content/CVPR2024/html/Lin_MLP_Can_Be_A_Good_Transformer_Learner_CVPR_2024_paper.html)|![29486](https://github.com/HeChengHui/CVPR2024/assets/84503515/1aab8ad2-a8dc-46f5-b009-655b7b03d4eb)| [![GitHub](https://img.shields.io/github/stars/sihaoevery/lambda_vit?style=social)](https://github.com/sihaoevery/lambda_vit)<br> [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=sfBIZXsrzgc)
| [RMT: Retentive Networks Meet Vision Transformers ](https://openaccess.thecvf.com/content/CVPR2024/html/Fan_RMT_Retentive_Networks_Meet_Vision_Transformers_CVPR_2024_paper.html)| ![Poster](https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202024/30086.png?t=1716465605.0185454) | [![GitHub](https://img.shields.io/github/stars/qhfan/RMT?style=social)](https://github.com/qhfan/RMT)
|[TransNeXt: Robust Foveal Visual Perception for Vision Transformers](https://openaccess.thecvf.com/content/CVPR2024/html/Shi_TransNeXt_Robust_Foveal_Visual_Perception_for_Vision_Transformers_CVPR_2024_paper.html)| ![image](https://github.com/HeChengHui/CVPR2024/assets/84503515/ef4e1c42-b053-4945-9a50-55e39a380012)| [![GitHub](https://img.shields.io/github/stars/DaiShiResearch/TransNeXt?style=social)](https://github.com/DaiShiResearch/TransNeXt)<br> [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=MTv3QpNXMU8)

---

### Conv
|Title|Poster|Resources|Pic|
|------|------|------|------|
| ‚≠ê[Efficient Deformable ConvNets: Rethinking Dynamic and Sparse Operator for Vision Applications ](https://openaccess.thecvf.com/content/CVPR2024/html/Xiong_Efficient_Deformable_ConvNets_Rethinking_Dynamic_and_Sparse_Operator_for_Vision_CVPR_2024_paper.html)|![Poster](https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202024/31637.png?t=1717602259.6634343) | [![GitHub](https://img.shields.io/github/stars/OpenGVLab/DCNv4?style=social)](https://github.com/OpenGVLab/DCNv4)
|  [InceptionNeXt: When Inception Meets ConvNeXt ](https://openaccess.thecvf.com/content/CVPR2024/html/Yu_InceptionNeXt_When_Inception_Meets_ConvNeXt_CVPR_2024_paper.html)| ![Poster](https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202024/29336.png?t=1717392635.727063) | [![GitHub](https://img.shields.io/github/stars/sail-sg/inceptionnext?style=social)](https://github.com/sail-sg/inceptionnext)<br> [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=FqR47ON8tdg)

---

### Vision Foundation Model
|Title|Poster|Resources|Pic|
|------|------|------|------|
| ‚≠ê[General Object Foundation Model for Images and Videos at Scale ](https://openaccess.thecvf.com/content/CVPR2024/html/Wu_General_Object_Foundation_Model_for_Images_and_Videos_at_Scale_CVPR_2024_paper.html)| ![Poster](https://github.com/HeChengHui/CVPR2024/blob/main/Papers/Topics/Vision%20LLM/assets/29939.png) | [![GitHub](https://img.shields.io/github/stars/FoundationVision/GLEE?style=social)](https://github.com/FoundationVision/GLEE)<br> [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=PSVhfTPx0GQ)
| [Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks ](https://openaccess.thecvf.com/content/CVPR2024/html/Xiao_Florence-2_Advancing_a_Unified_Representation_for_a_Variety_of_Vision_CVPR_2024_paper.html)| ![Poster](https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202024/30529.png?t=1717455193.7819567) | [![HuggingFace](https://img.shields.io/badge/hugging_face-1?style=for-the-badge&logo=huggingface&logoColor=%23FFD21E&color=white)](https://huggingface.co/microsoft/Florence-2-large) <br> [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=cOlyA00K1ec)
| [AM-RADIO: Agglomerative Vision Foundation Model Reduce All Domains Into One ](https://openaccess.thecvf.com/content/CVPR2024/html/Ranzinger_AM-RADIO_Agglomerative_Vision_Foundation_Model_Reduce_All_Domains_Into_One_CVPR_2024_paper.html)| ![30113](https://github.com/HeChengHui/CVPR2024/assets/84503515/dcd7a422-999a-410f-ad7d-48a23d71d076)| [![GitHub](https://img.shields.io/github/stars/NVlabs/RADIO?style=social)](https://github.com/NVlabs/RADIO)
|  [Alpha-CLIP: A CLIP Model Focusing on Wherever You Want ](https://openaccess.thecvf.com/content/CVPR2024/html/Sun_Alpha-CLIP_A_CLIP_Model_Focusing_on_Wherever_You_Want_CVPR_2024_paper.html)| ![31492](https://github.com/HeChengHui/CVPR2024/assets/84503515/3f1c76a9-f7f2-4e1d-8d87-cfe7a1b6e2e6)| [![GitHub](https://img.shields.io/github/stars/SunzeY/AlphaCLIP?style=social)](https://github.com/SunzeY/AlphaCLIP)<br> [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=QCEIKPZpZz0)
| [Aligning and Prompting Everything All at Once for Universal Visual Perception ](https://openaccess.thecvf.com/content/CVPR2024/html/Shen_Aligning_and_Prompting_Everything_All_at_Once_for_Universal_Visual_CVPR_2024_paper.html)| ![29477](https://github.com/HeChengHui/CVPR2024/assets/84503515/5ee3dbe9-9a52-44f1-9c8c-24b5680d64fe)| [![GitHub](https://img.shields.io/github/stars/shenyunhang/APE?style=social)](https://github.com/shenyunhang/APE)
