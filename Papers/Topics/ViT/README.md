### ViT
|Title|Poster|Resources|Pic|
|------|------|------|------|
| [RMT: Retentive Networks Meet Vision Transformers ](https://openaccess.thecvf.com/content/CVPR2024/html/Fan_RMT_Retentive_Networks_Meet_Vision_Transformers_CVPR_2024_paper.html)| ![Poster](https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202024/30086.png?t=1716465605.0185454) | [![GitHub](https://img.shields.io/github/stars/qhfan/RMT?style=social)](https://github.com/qhfan/RMT)

---

### Conv
|Title|Poster|Resources|Pic|
|------|------|------|------|
| ⭐[Efficient Deformable ConvNets: Rethinking Dynamic and Sparse Operator for Vision Applications ](https://openaccess.thecvf.com/content/CVPR2024/html/Xiong_Efficient_Deformable_ConvNets_Rethinking_Dynamic_and_Sparse_Operator_for_Vision_CVPR_2024_paper.html)|![Poster](https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202024/31637.png?t=1717602259.6634343) | [![GitHub](https://img.shields.io/github/stars/OpenGVLab/DCNv4?style=social)](https://github.com/OpenGVLab/DCNv4)
|  [InceptionNeXt: When Inception Meets ConvNeXt ](https://openaccess.thecvf.com/content/CVPR2024/html/Yu_InceptionNeXt_When_Inception_Meets_ConvNeXt_CVPR_2024_paper.html)| ![Poster](https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202024/29336.png?t=1717392635.727063) | [![GitHub](https://img.shields.io/github/stars/sail-sg/inceptionnext?style=social)](https://github.com/sail-sg/inceptionnext)<br> [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=FqR47ON8tdg)

---

### Vision Foundation Model
|Title|Poster|Resources|Pic|
|------|------|------|------|
| ⭐[General Object Foundation Model for Images and Videos at Scale ](https://openaccess.thecvf.com/content/CVPR2024/html/Wu_General_Object_Foundation_Model_for_Images_and_Videos_at_Scale_CVPR_2024_paper.html)| ![Poster](https://github.com/HeChengHui/CVPR2024/blob/main/Papers/Topics/Vision%20LLM/assets/29939.png) | [![GitHub](https://img.shields.io/github/stars/FoundationVision/GLEE?style=social)](https://github.com/FoundationVision/GLEE)<br> [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=PSVhfTPx0GQ)
| [Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks ](https://openaccess.thecvf.com/content/CVPR2024/html/Xiao_Florence-2_Advancing_a_Unified_Representation_for_a_Variety_of_Vision_CVPR_2024_paper.html)| ![Poster](https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202024/30529.png?t=1717455193.7819567) | [![HuggingFace](https://img.shields.io/badge/hugging_face-1?style=for-the-badge&logo=huggingface&logoColor=%23FFD21E&color=white)](https://huggingface.co/microsoft/Florence-2-large) <br> [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=cOlyA00K1ec)
| [AM-RADIO: Agglomerative Vision Foundation Model Reduce All Domains Into One ](https://openaccess.thecvf.com/content/CVPR2024/html/Ranzinger_AM-RADIO_Agglomerative_Vision_Foundation_Model_Reduce_All_Domains_Into_One_CVPR_2024_paper.html)| ![30113](https://github.com/HeChengHui/CVPR2024/assets/84503515/dcd7a422-999a-410f-ad7d-48a23d71d076)| [![GitHub](https://img.shields.io/github/stars/NVlabs/RADIO?style=social)](https://github.com/NVlabs/RADIO)
|  [Alpha-CLIP: A CLIP Model Focusing on Wherever You Want ](https://openaccess.thecvf.com/content/CVPR2024/html/Sun_Alpha-CLIP_A_CLIP_Model_Focusing_on_Wherever_You_Want_CVPR_2024_paper.html)| ![31492](https://github.com/HeChengHui/CVPR2024/assets/84503515/3f1c76a9-f7f2-4e1d-8d87-cfe7a1b6e2e6)| [![GitHub](https://img.shields.io/github/stars/SunzeY/AlphaCLIP?style=social)](https://github.com/SunzeY/AlphaCLIP)<br> [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=QCEIKPZpZz0)
