### Full list [here](https://cvpr.thecvf.com/Conferences/2024/workshop-list)
<p align="center">
    <img src="https://i.imgur.com/waxVImv.png" alt="Oryx Video-ChatGPT">
</p>
  
The following table only shows workshops with relevant papers + possible code available either shown here or stated in the papers. If all papers in a workshop are present in the [main conference](https://github.com/HeChengHui/CVPR2024/tree/main/Papers), the workshop would not be mentioned here.  
  
***NOTE***: Unders the Resources column, the first link shows all the accepted papers in that workshop, followed by any other available resources I can find.

| Workshops | Keywords | Resources | Codes |
|:-------------------:|:-------------------:|:-------------------:|:-------------------:|    
|9th New Trends in Image Restoration and Enhancement Workshop and Challenges| <ul><li>Low Light Image Enhancement</li><li>Image Shadow Removal</li><li>Blind Enhancement of Compressed Image</li><li>Image Restoration In The Wild</li><li>Image Super-Restoration (x4)</li><li>Dense and Non-Homogeneous Dehazing</li><li>Efficient Super-Resolution</li></ul>|[![Open Access](https://a11ybadges.com/badge?logo=openaccess)](https://openaccess.thecvf.com/CVPR2024_workshops/NTIRE)||
|8th AI City Challenge| <ul><li>Multi-Camera People Tracking<li>Naturalistic Driving Action Recognition| [![Open Access](https://a11ybadges.com/badge?logo=openaccess)](https://openaccess.thecvf.com/CVPR2024_workshops/AICity) <br> [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=yVshNGpb4Yc)| Compile:[![GitHub](https://img.shields.io/github/stars/NVIDIAAICITYCHALLENGE/2024AICITY_Code_From_Top_Teams?style=social)](https://github.com/NVIDIAAICITYCHALLENGE/2024AICITY_Code_From_Top_Teams)|
| Efficient Large Vision Models | <ul><li>Efficient ViT<li>Segmentation| [![Open Access](https://a11ybadges.com/badge?logo=openaccess)](https://openaccess.thecvf.com/CVPR2024_workshops/eLVM) <br> [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=lirjgIgngUE)||
|Multimodal Algorithmic Reasoning| <ul><li>Agent| [![Open Access](https://a11ybadges.com/badge?logo=openaccess)](https://openaccess.thecvf.com/CVPR2024_workshops/MAR) <br> [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=LooLbLs3O_Y)|Best Paper: [![GitHub](https://img.shields.io/github/stars/Berkeley-NLP/Agent-Eval-Refine?style=social)](https://github.com/Berkeley-NLP/Agent-Eval-Refine)|
| CVPR 2024 Biometrics Workshop |<ul><li>Face Image QA<li>Tatto Retrival<li>RGB-D FR| [![Open Access](https://a11ybadges.com/badge?logo=openaccess)](https://openaccess.thecvf.com/CVPR2024_workshops/BIOMET)|GraFIQs: [![GitHub](https://img.shields.io/github/stars/jankolf/GraFIQs?style=social)](https://github.com/jankolf/GraFIQs)<br>TattTRN: [![GitHub](https://img.shields.io/github/stars/ljsoler/TattTRN?style=social)](https://github.com/ljsoler/TattTRN)|
| The 3rd Workshop on Transformers for Vision | <ul><li>ViT| [![Google Sites](https://img.shields.io/badge/google_sites-test?style=for-the-badge&logo=google&logoColor=white&color=%23445cb6)](https://sites.google.com/view/t4v-cvpr24/papers?authuser=0)||
|2024 VizWiz Grand Challenge Workshop|<ul><li>Visual Question Answering<li>VQA Grounding| [![WordPress](https://a11ybadges.com/badge?logo=wordpress)](https://vizwiz.org/workshops/2024-vizwiz-grand-challenge-workshop/)<br>Full: [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=4f28uRG7klw)<br>Channel: [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/@vizwiz3911/videos)<br>[![ChatGPT](https://img.shields.io/badge/chatGPT-74aa9c?style=for-the-badge&logo=openai&logoColor=white)](https://www.youtube.com/watch?v=So_4tCCCLpc)<br> [![My Notes](https://a11ybadges.com/badge?logo=book-open&text=My_Notes)](https://github.com/HeChengHui/CVPR2024/blob/main/Materials/18June/18%20June_CVPR2024.pdf)||
|2nd Face Recognition Challenge in the Era of Synthetic Data (FRCSyn) |<ul><li>Synthetic Data for FR| [![arXiv](https://img.shields.io/badge/arXiv-2404.10378-b31b1b.svg?style=for-the-badge)](https://arxiv.org/pdf/2404.10378)<br><details><summary>TLDR</summary>1. Unconstrained synthetic data generally led to better performance than constrained synthetic data.<br>2. Combining synthetic and real data often yielded the best results, particularly in bias mitigation.<br>3. In some cases, unlimited synthetic data outperformed limited real data.<br>4. Most top teams used established methodologies like ResNet or IResNet backbones and AdaFace or ArcFace loss functions.<br>5. DCFace was widely used for synthetic data generation, often combined with other methods.<br>6. The challenge demonstrated significant improvements compared to the 1st edition, showcasing the potential of synthetic data in face recognition.</details>||
| 2nd Workshop on What is Next in Multimodal Foundation Models?| <ul><li>Multi Modal LLM for image & video| [![Google Sites](https://img.shields.io/badge/google_sites-test?style=for-the-badge&logo=google&logoColor=white&color=%23445cb6)](https://sites.google.com/view/2nd-mmfm-workshop/program?authuser=0) <br> [![GitHub](https://img.shields.io/github/stars/EliSchwartz/MMFM24?style=social)](https://github.com/EliSchwartz/MMFM24)||
|1st Workshop on Test-Time Adaptation|<ul><li>Test-Time Adaptation for Object Detection and Action Recognition| [![Github Pages](https://img.shields.io/badge/github%20pages-121013?style=for-the-badge&logo=github&logoColor=white)](https://tta-cvpr2024.github.io/papers.html)<br>[![Google Slides](https://img.shields.io/badge/Google_slides-test?style=for-the-badge&logo=googleslides&logoColor=%23FBBC04&color=white)](https://docs.google.com/presentation/d/1Qj3QxE-GMILEkXLKmcCQsyVOFqn4nB4sUsX9ZvKgLpI/edit#slide=id.g2e4bb78fbc4_1_0)|IoU-filter: [![GitHub](https://img.shields.io/github/stars/XiaoqianRuan1/IoU-filter?style=social)](https://github.com/XiaoqianRuan1/IoU-filter)|
|GAZE 2024|<ul><li>Gaze Estimation| [![Github Pages](https://img.shields.io/badge/github%20pages-121013?style=for-the-badge&logo=github&logoColor=white)](https://gazeworkshop.github.io/2024/) <br> [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=Vgrgi53efkE)|stage:<br>[![GitHub](https://img.shields.io/github/stars/jswati31/stage?style=social)](https://github.com/jswati31/stage)|
| The 3rd Explainable AI for Computer Vision (XAI4CV) Workshop | <ul><li>Explainablility for CNN & ViT| [Papers + Posters](https://xai4cv.github.io/workshop_cvpr24) <br> [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=o2YmzPXtAgc)|lvlm-interpret: [![GitHub](https://img.shields.io/github/stars/IntelLabs/lvlm-interpret?style=social)](https://github.com/IntelLabs/lvlm-interpret)| 
|Embedded Vision Workshop| <ul><li>Video Object Detection for Embedded Systems| [![Open Access](https://a11ybadges.com/badge?logo=openaccess)](https://openaccess.thecvf.com/CVPR2024_workshops/EVW) <br> [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=HlolzdAvzgM)| MR2-ByteTrack:<br>[![GitHub](https://img.shields.io/github/stars/Bomps4/Multi_Resolution_Rescored_ByteTrack?style=social)](https://github.com/Bomps4/Multi_Resolution_Rescored_ByteTrack)


[comment]: <> (googleslide simple icons not working)
