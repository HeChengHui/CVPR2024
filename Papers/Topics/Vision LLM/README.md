|Title|Poster|Resources|Pic|
|------|------|------|------|
| ‚≠ê[mPLUG-Owl2: Revolutionizing Multi-modal Large Language Model with Modality Collaboration ](https://openaccess.thecvf.com/content/CVPR2024/html/Ye_mPLUG-Owl2_Revolutionizing_Multi-modal_Large_Language_Model_with_Modality_Collaboration_CVPR_2024_paper.html)| ![31761](https://github.com/HeChengHui/CVPR2024/assets/84503515/acd87e2e-f0e8-46cc-9ddb-a21a981b195e)| [![GitHub](https://img.shields.io/github/stars/X-PLUG/mPLUG-Owl?style=social)](https://github.com/X-PLUG/mPLUG-Owl)
| ‚≠ê[Chat-UniVi: Unified Visual Representation Empowers Large Language Models with Image and Video Understanding ](https://openaccess.thecvf.com/content/CVPR2024/html/Jin_Chat-UniVi_Unified_Visual_Representation_Empowers_Large_Language_Models_with_Image_CVPR_2024_paper.html)| ![31713](https://github.com/HeChengHui/CVPR2024/assets/84503515/ad09e3da-deb9-482b-ba6f-45bd213ce3e4)| [![GitHub](https://img.shields.io/github/stars/PKU-YuanGroup/Chat-UniVi?style=social)](https://github.com/PKU-YuanGroup/Chat-UniVi)<br> [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=qlaFdxcPUIY)
| ‚≠ê[Honeybee: Locality-enhanced Projector for Multimodal LLM ](https://openaccess.thecvf.com/content/CVPR2024/html/Cha_Honeybee_Locality-enhanced_Projector_for_Multimodal_LLM_CVPR_2024_paper.html)|![29641](https://github.com/HeChengHui/CVPR2024/assets/84503515/1378d9f5-3750-4911-8824-ff45adb3e68a) | [![GitHub](https://img.shields.io/github/stars/kakaobrain/honeybee?style=social)](https://github.com/kakaobrain/honeybee)<br> [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=auVcQ4ZhEb0)
| ‚≠ê[Question Aware Vision Transformer for Multimodal Reasoning ](https://openaccess.thecvf.com/content/CVPR2024/html/Ganz_Question_Aware_Vision_Transformer_for_Multimodal_Reasoning_CVPR_2024_paper.html)| ![31218](https://github.com/HeChengHui/CVPR2024/assets/84503515/d7734701-1a24-4fbd-8c71-f33780b1d167)| [![GitHub](https://img.shields.io/github/stars/amazon-science/QA-ViT?style=social)](https://github.com/amazon-science/QA-ViT)<br> [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=OCM-O60wxJg)
| [ViTamin: Designing Scalable Vision Models in the Vision-Language Era ](https://openaccess.thecvf.com/content/CVPR2024/html/Chen_ViTamin_Designing_Scalable_Vision_Models_in_the_Vision-Language_Era_CVPR_2024_paper.html)| | [![GitHub](https://img.shields.io/github/stars/Beckschen/ViTamin?style=social)](https://github.com/Beckschen/ViTamin)| ![Pic](https://github.com/HeChengHui/CVPR2024/blob/main/Papers/Topics/Vision%20LLM/assets/WhatsApp%20Image%202024-07-04%20at%2022.49.48.jpeg)
| ‚≠ê[Jack of All Tasks Master of Many: Designing General-Purpose Coarse-to-Fine Vision-Language Model ](https://openaccess.thecvf.com/content/CVPR2024/html/Pramanick_Jack_of_All_Tasks_Master_of_Many_Designing_General-Purpose_Coarse-to-Fine_CVPR_2024_paper.html)| |[![Github Pages](https://img.shields.io/badge/github%20pages-121013?style=for-the-badge&logo=github&logoColor=white)](https://shramanpramanick.github.io/VistaLLM/)|![Pic](https://github.com/HeChengHui/CVPR2024/blob/main/Papers/Topics/Vision%20LLM/assets/WhatsApp%20Image%202024-07-05%20at%2014.54.55.jpeg)

---

### Grounding
|Title|Poster|Resources|Pic|
|------|------|------|------|
| ‚≠ê[LISA: Reasoning Segmentation via Large Language Model ](https://openaccess.thecvf.com/content/CVPR2024/html/Lai_LISA_Reasoning_Segmentation_via_Large_Language_Model_CVPR_2024_paper.html)| ![30109](https://github.com/HeChengHui/CVPR2024/assets/84503515/81b0c9e1-9da7-49fa-81f8-5c4f928c49f8)| [![GitHub](https://img.shields.io/github/stars/dvlab-research/LISA?style=social)](https://github.com/dvlab-research/LISA)
| [ViP-LLaVA: Making Large Multimodal Models Understand Arbitrary Visual Prompts ](https://openaccess.thecvf.com/content/CVPR2024/html/Cai_ViP-LLaVA_Making_Large_Multimodal_Models_Understand_Arbitrary_Visual_Prompts_CVPR_2024_paper.html)|![29580](https://github.com/HeChengHui/CVPR2024/assets/84503515/47b988d2-546d-451d-b713-2cf8b5b287cf)| [![GitHub](https://img.shields.io/github/stars/WisconsinAIVision/ViP-LLaVA?style=social)](https://github.com/WisconsinAIVision/ViP-LLaVA)<br> [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=j_l1bRQouzc)
|  [GLaMM: Pixel Grounding Large Multimodal Model ](https://openaccess.thecvf.com/content/CVPR2024/html/Rasheed_GLaMM_Pixel_Grounding_Large_Multimodal_Model_CVPR_2024_paper.html)| | [![GitHub](https://img.shields.io/github/stars/mbzuai-oryx/groundingLMM?style=social)](https://github.com/mbzuai-oryx/groundingLMM)| ![Pic](https://github.com/HeChengHui/CVPR2024/blob/main/Papers/Topics/Vision%20LLM/assets/WhatsApp%20Image%202024-07-04%20at%2023.02.20.jpeg)
| [V?: Guided Visual Search as a Core Mechanism in Multimodal LLMs ](https://openaccess.thecvf.com/content/CVPR2024/html/Wu_V_Guided_Visual_Search_as_a_Core_Mechanism_in_Multimodal_CVPR_2024_paper.html)| ![31596](https://github.com/HeChengHui/CVPR2024/assets/84503515/a0cb1820-172a-4fb9-8dc5-d894aed5af30)| [![GitHub](https://img.shields.io/github/stars/penghao-wu/vstar?style=social)](https://github.com/penghao-wu/vstar)<br> [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=vlsUqJhiCns)
| [See Say and Segment: Teaching LMMs to Overcome False Premises ](https://openaccess.thecvf.com/content/CVPR2024/html/Wu_See_Say_and_Segment_Teaching_LMMs_to_Overcome_False_Premises_CVPR_2024_paper.html)| ![31231](https://github.com/HeChengHui/CVPR2024/assets/84503515/c00821b1-e35d-49f1-b53b-bc2c327786aa)| [![GitHub](https://img.shields.io/github/stars/see-say-segment/sesame?style=social)](https://github.com/see-say-segment/sesame)<br> [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=-TXCR-m3MJ4)
| [RegionGPT: Towards Region Understanding Vision Language Model ](https://openaccess.thecvf.com/content/CVPR2024/html/Guo_RegionGPT_Towards_Region_Understanding_Vision_Language_Model_CVPR_2024_paper.html)|![31126](https://github.com/HeChengHui/CVPR2024/assets/84503515/65e0ecbf-32ba-4980-876d-1abde8259be5)| [![Github Pages](https://img.shields.io/badge/github%20pages-121013?style=for-the-badge&logo=github&logoColor=white)](https://guoqiushan.github.io/regiongpt.github.io/) <br> [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=C4IHpFrUb9U)

---

### Agent
|Title|Poster|Resources|Pic|
|------|------|------|------|
| [AssistGUI: Task-Oriented PC Graphical User Interface Automation ](https://openaccess.thecvf.com/content/CVPR2024/html/Gao_AssistGUI_Task-Oriented_PC_Graphical_User_Interface_Automation_CVPR_2024_paper.html)|| [![GitHub](https://img.shields.io/github/stars/showlab/assistgui?style=social)](https://github.com/showlab/assistgui)| ![Pic](https://github.com/HeChengHui/CVPR2024/blob/main/Papers/Topics/Vision%20LLM/assets/WhatsApp%20Image%202024-07-05%20at%2000.04.23.jpeg)

---

### Video-Language
|Title|Poster|Resources|Pic|
|------|------|------|------|
| ‚≠ê[Koala: Key Frame-Conditioned Long Video-LLM ](https://openaccess.thecvf.com/content/CVPR2024/html/Tan_Koala_Key_Frame-Conditioned_Long_Video-LLM_CVPR_2024_paper.html)|![31782](https://github.com/HeChengHui/CVPR2024/assets/84503515/6f465e3e-ec5a-4973-ba8b-a74c5179fa0b)| [![GitHub](https://img.shields.io/github/stars/rxtan2/Koala-video-llm?style=social)](https://github.com/rxtan2/Koala-video-llm)<br> [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=mN9GT9b0BTw)
|‚≠ê[ VTimeLLM: Empower LLM to Grasp Video Moments ](https://openaccess.thecvf.com/content/CVPR2024/html/Huang_VTimeLLM_Empower_LLM_to_Grasp_Video_Moments_CVPR_2024_paper.html)| ![31139](https://github.com/HeChengHui/CVPR2024/assets/84503515/49d63950-9fb8-41c4-87de-746eaac74a48)| [![GitHub](https://img.shields.io/github/stars/huangb23/VTimeLLM?style=social)](https://github.com/huangb23/VTimeLLM)<br> [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=enbP-Up3Hnw)
| [MA-LMM: Memory-Augmented Large Multimodal Model for Long-Term Video Understanding ](https://openaccess.thecvf.com/content/CVPR2024/html/He_MA-LMM_Memory-Augmented_Large_Multimodal_Model_for_Long-Term_Video_Understanding_CVPR_2024_paper.html)| ![30043](https://github.com/HeChengHui/CVPR2024/assets/84503515/92ccb2bd-5d83-41af-81e3-4d708bc637e0)| [![GitHub](https://img.shields.io/github/stars/boheumd/MA-LMM?style=social)](https://github.com/boheumd/MA-LMM)<br> [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=wwIGNj98Hc0)
| [Distilling Vision-Language Models on Millions of Videos ](https://openaccess.thecvf.com/content/CVPR2024/html/Zhao_Distilling_Vision-Language_Models_on_Millions_of_Videos_CVPR_2024_paper.html)| ![30028](https://github.com/HeChengHui/CVPR2024/assets/84503515/4f9c0fa3-4468-4e0e-99bb-f4f69eb3db94)| [![Github Pages](https://img.shields.io/badge/github%20pages-121013?style=for-the-badge&logo=github&logoColor=white)](https://zhaoyue-zephyrus.github.io/video-instruction-tuning/)

---

### Misinformation Detection
|Title|Poster|Resources|Pic|
|------|------|------|------|
| [SNIFFER: Multimodal Large Language Model for Explainable Out-of-Context Misinformation Detection ](https://openaccess.thecvf.com/content/CVPR2024/html/Qi_SNIFFER_Multimodal_Large_Language_Model_for_Explainable_Out-of-Context_Misinformation_Detection_CVPR_2024_paper.html)| <img width="1731" alt="31274" src="https://github.com/HeChengHui/CVPR2024/assets/84503515/99b1b5e1-4ae2-4732-a433-9521978078c6"> | [![Github Pages](https://img.shields.io/badge/github%20pages-121013?style=for-the-badge&logo=github&logoColor=white)](https://pengqi.site/Sniffer/) <br> [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=zPTZnz9nhlI)

---

### Hallucination Mitigation
|Title|Poster|Resources|Pic|
|------|------|------|------|
| ‚≠ê[OPERA: Alleviating Hallucination in Multi-Modal Large Language Models via Over-Trust Penalty and Retrospection-Allocation ](https://openaccess.thecvf.com/content/CVPR2024/html/Huang_OPERA_Alleviating_Hallucination_in_Multi-Modal_Large_Language_Models_via_Over-Trust_CVPR_2024_paper.html)|![30961](https://github.com/HeChengHui/CVPR2024/assets/84503515/ac08dda2-a0ac-4c58-b7ab-38e6d5437a9a)| [![GitHub](https://img.shields.io/github/stars/shikiw/OPERA?style=social)](https://github.com/shikiw/OPERA)
| ‚≠ê[Mitigating Object Hallucinations in Large Vision-Language Models through Visual Contrastive Decoding ](https://openaccess.thecvf.com/content/CVPR2024/html/Leng_Mitigating_Object_Hallucinations_in_Large_Vision-Language_Models_through_Visual_Contrastive_CVPR_2024_paper.html)||[![GitHub](https://img.shields.io/github/stars/DAMO-NLP-SG/VCD?style=social)](https://github.com/DAMO-NLP-SG/VCD)

---
### Benchmark
|Title|Poster|Resources|Pic|
|------|------|------|------|
| üèÜ‚≠ê[MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI ](https://openaccess.thecvf.com/content/CVPR2024/html/Yue_MMMU_A_Massive_Multi-discipline_Multimodal_Understanding_and_Reasoning_Benchmark_for_CVPR_2024_paper.html)|![31040](https://github.com/HeChengHui/CVPR2024/assets/84503515/9549f29c-5395-4ffd-80ae-8c512b76565e)| [![GitHub](https://img.shields.io/github/stars/MMMU-Benchmark/MMMU?style=social)](https://github.com/MMMU-Benchmark/MMMU)
