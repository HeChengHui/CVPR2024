|Title|Poster|Resources|Pic|
|------|------|------|------|
| [ViP-LLaVA: Making Large Multimodal Models Understand Arbitrary Visual Prompts ](https://openaccess.thecvf.com/content/CVPR2024/html/Cai_ViP-LLaVA_Making_Large_Multimodal_Models_Understand_Arbitrary_Visual_Prompts_CVPR_2024_paper.html)|![29580](https://github.com/HeChengHui/CVPR2024/assets/84503515/47b988d2-546d-451d-b713-2cf8b5b287cf)| [![GitHub](https://img.shields.io/github/stars/WisconsinAIVision/ViP-LLaVA?style=social)](https://github.com/WisconsinAIVision/ViP-LLaVA)<br> [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=j_l1bRQouzc)
| [ViTamin: Designing Scalable Vision Models in the Vision-Language Era ](https://openaccess.thecvf.com/content/CVPR2024/html/Chen_ViTamin_Designing_Scalable_Vision_Models_in_the_Vision-Language_Era_CVPR_2024_paper.html)| | [![GitHub](https://img.shields.io/github/stars/Beckschen/ViTamin?style=social)](https://github.com/Beckschen/ViTamin)| ![Pic](https://github.com/HeChengHui/CVPR2024/blob/main/Papers/Topics/Vision%20LLM/assets/WhatsApp%20Image%202024-07-04%20at%2022.49.48.jpeg)
|  [GLaMM: Pixel Grounding Large Multimodal Model ](https://openaccess.thecvf.com/content/CVPR2024/html/Rasheed_GLaMM_Pixel_Grounding_Large_Multimodal_Model_CVPR_2024_paper.html)| | [![GitHub](https://img.shields.io/github/stars/mbzuai-oryx/groundingLMM?style=social)](https://github.com/mbzuai-oryx/groundingLMM)| ![Pic](https://github.com/HeChengHui/CVPR2024/blob/main/Papers/Topics/Vision%20LLM/assets/WhatsApp%20Image%202024-07-04%20at%2023.02.20.jpeg)
