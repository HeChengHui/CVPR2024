|Title|Poster|Resources|Pic|
|------|------|------|------|
| ⭐[mPLUG-Owl2: Revolutionizing Multi-modal Large Language Model with Modality Collaboration ](https://openaccess.thecvf.com/content/CVPR2024/html/Ye_mPLUG-Owl2_Revolutionizing_Multi-modal_Large_Language_Model_with_Modality_Collaboration_CVPR_2024_paper.html)| ![31761](https://github.com/HeChengHui/CVPR2024/assets/84503515/acd87e2e-f0e8-46cc-9ddb-a21a981b195e)| [![GitHub](https://img.shields.io/github/stars/X-PLUG/mPLUG-Owl?style=social)](https://github.com/X-PLUG/mPLUG-Owl)
| [ViTamin: Designing Scalable Vision Models in the Vision-Language Era ](https://openaccess.thecvf.com/content/CVPR2024/html/Chen_ViTamin_Designing_Scalable_Vision_Models_in_the_Vision-Language_Era_CVPR_2024_paper.html)| | [![GitHub](https://img.shields.io/github/stars/Beckschen/ViTamin?style=social)](https://github.com/Beckschen/ViTamin)| ![Pic](https://github.com/HeChengHui/CVPR2024/blob/main/Papers/Topics/Vision%20LLM/assets/WhatsApp%20Image%202024-07-04%20at%2022.49.48.jpeg)

---

### Grounding
|Title|Poster|Resources|Pic|
|------|------|------|------|
| [ViP-LLaVA: Making Large Multimodal Models Understand Arbitrary Visual Prompts ](https://openaccess.thecvf.com/content/CVPR2024/html/Cai_ViP-LLaVA_Making_Large_Multimodal_Models_Understand_Arbitrary_Visual_Prompts_CVPR_2024_paper.html)|![29580](https://github.com/HeChengHui/CVPR2024/assets/84503515/47b988d2-546d-451d-b713-2cf8b5b287cf)| [![GitHub](https://img.shields.io/github/stars/WisconsinAIVision/ViP-LLaVA?style=social)](https://github.com/WisconsinAIVision/ViP-LLaVA)<br> [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=j_l1bRQouzc)
|  [GLaMM: Pixel Grounding Large Multimodal Model ](https://openaccess.thecvf.com/content/CVPR2024/html/Rasheed_GLaMM_Pixel_Grounding_Large_Multimodal_Model_CVPR_2024_paper.html)| | [![GitHub](https://img.shields.io/github/stars/mbzuai-oryx/groundingLMM?style=social)](https://github.com/mbzuai-oryx/groundingLMM)| ![Pic](https://github.com/HeChengHui/CVPR2024/blob/main/Papers/Topics/Vision%20LLM/assets/WhatsApp%20Image%202024-07-04%20at%2023.02.20.jpeg)
| [V?: Guided Visual Search as a Core Mechanism in Multimodal LLMs ](https://openaccess.thecvf.com/content/CVPR2024/html/Wu_V_Guided_Visual_Search_as_a_Core_Mechanism_in_Multimodal_CVPR_2024_paper.html)| ![31596](https://github.com/HeChengHui/CVPR2024/assets/84503515/a0cb1820-172a-4fb9-8dc5-d894aed5af30)| [![GitHub](https://img.shields.io/github/stars/penghao-wu/vstar?style=social)](https://github.com/penghao-wu/vstar)<br> [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=vlsUqJhiCns)
| [See Say and Segment: Teaching LMMs to Overcome False Premises ](https://openaccess.thecvf.com/content/CVPR2024/html/Wu_See_Say_and_Segment_Teaching_LMMs_to_Overcome_False_Premises_CVPR_2024_paper.html)| ![31231](https://github.com/HeChengHui/CVPR2024/assets/84503515/c00821b1-e35d-49f1-b53b-bc2c327786aa)| [![GitHub](https://img.shields.io/github/stars/see-say-segment/sesame?style=social)](https://github.com/see-say-segment/sesame)<br> [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=-TXCR-m3MJ4)

---

### Agent
|Title|Poster|Resources|Pic|
|------|------|------|------|
| [AssistGUI: Task-Oriented PC Graphical User Interface Automation ](https://openaccess.thecvf.com/content/CVPR2024/html/Gao_AssistGUI_Task-Oriented_PC_Graphical_User_Interface_Automation_CVPR_2024_paper.html)|| [![GitHub](https://img.shields.io/github/stars/showlab/assistgui?style=social)](https://github.com/showlab/assistgui)| ![Pic](https://github.com/HeChengHui/CVPR2024/blob/main/Papers/Topics/Vision%20LLM/assets/WhatsApp%20Image%202024-07-05%20at%2000.04.23.jpeg)

---

### Video-Language
|Title|Poster|Resources|Pic|
|------|------|------|------|
| [MA-LMM: Memory-Augmented Large Multimodal Model for Long-Term Video Understanding ](https://openaccess.thecvf.com/content/CVPR2024/html/He_MA-LMM_Memory-Augmented_Large_Multimodal_Model_for_Long-Term_Video_Understanding_CVPR_2024_paper.html)| ![30043](https://github.com/HeChengHui/CVPR2024/assets/84503515/92ccb2bd-5d83-41af-81e3-4d708bc637e0)| [![GitHub](https://img.shields.io/github/stars/boheumd/MA-LMM?style=social)](https://github.com/boheumd/MA-LMM)<br> [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=wwIGNj98Hc0)
| [Distilling Vision-Language Models on Millions of Videos ](https://openaccess.thecvf.com/content/CVPR2024/html/Zhao_Distilling_Vision-Language_Models_on_Millions_of_Videos_CVPR_2024_paper.html)| ![30028](https://github.com/HeChengHui/CVPR2024/assets/84503515/4f9c0fa3-4468-4e0e-99bb-f4f69eb3db94)| [![Github Pages](https://img.shields.io/badge/github%20pages-121013?style=for-the-badge&logo=github&logoColor=white)](https://zhaoyue-zephyrus.github.io/video-instruction-tuning/)

---

### Misinformation Detection
|Title|Poster|Resources|Pic|
|------|------|------|------|
| [SNIFFER: Multimodal Large Language Model for Explainable Out-of-Context Misinformation Detection ](https://openaccess.thecvf.com/content/CVPR2024/html/Qi_SNIFFER_Multimodal_Large_Language_Model_for_Explainable_Out-of-Context_Misinformation_Detection_CVPR_2024_paper.html)| <img width="1731" alt="31274" src="https://github.com/HeChengHui/CVPR2024/assets/84503515/99b1b5e1-4ae2-4732-a433-9521978078c6"> | [![Github Pages](https://img.shields.io/badge/github%20pages-121013?style=for-the-badge&logo=github&logoColor=white)](https://pengqi.site/Sniffer/) <br> [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=zPTZnz9nhlI)

---


### Hallucination Mitigation
|Title|Poster|Resources|Pic|
|------|------|------|------|
| ⭐[OPERA: Alleviating Hallucination in Multi-Modal Large Language Models via Over-Trust Penalty and Retrospection-Allocation ](https://openaccess.thecvf.com/content/CVPR2024/html/Huang_OPERA_Alleviating_Hallucination_in_Multi-Modal_Large_Language_Models_via_Over-Trust_CVPR_2024_paper.html)|![30961](https://github.com/HeChengHui/CVPR2024/assets/84503515/ac08dda2-a0ac-4c58-b7ab-38e6d5437a9a)| [![GitHub](https://img.shields.io/github/stars/shikiw/OPERA?style=social)](https://github.com/shikiw/OPERA)
