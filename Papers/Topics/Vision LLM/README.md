|Title|Poster|Resources|Pic|
|------|------|------|------|
| ‚≠ê[mPLUG-Owl2: Revolutionizing Multi-modal Large Language Model with Modality Collaboration ](https://openaccess.thecvf.com/content/CVPR2024/html/Ye_mPLUG-Owl2_Revolutionizing_Multi-modal_Large_Language_Model_with_Modality_Collaboration_CVPR_2024_paper.html)| ![31761](https://github.com/HeChengHui/CVPR2024/assets/84503515/acd87e2e-f0e8-46cc-9ddb-a21a981b195e)| [![GitHub](https://img.shields.io/github/stars/X-PLUG/mPLUG-Owl?style=social)](https://github.com/X-PLUG/mPLUG-Owl)
| [ViP-LLaVA: Making Large Multimodal Models Understand Arbitrary Visual Prompts ](https://openaccess.thecvf.com/content/CVPR2024/html/Cai_ViP-LLaVA_Making_Large_Multimodal_Models_Understand_Arbitrary_Visual_Prompts_CVPR_2024_paper.html)|![29580](https://github.com/HeChengHui/CVPR2024/assets/84503515/47b988d2-546d-451d-b713-2cf8b5b287cf)| [![GitHub](https://img.shields.io/github/stars/WisconsinAIVision/ViP-LLaVA?style=social)](https://github.com/WisconsinAIVision/ViP-LLaVA)<br> [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=j_l1bRQouzc)
| [ViTamin: Designing Scalable Vision Models in the Vision-Language Era ](https://openaccess.thecvf.com/content/CVPR2024/html/Chen_ViTamin_Designing_Scalable_Vision_Models_in_the_Vision-Language_Era_CVPR_2024_paper.html)| | [![GitHub](https://img.shields.io/github/stars/Beckschen/ViTamin?style=social)](https://github.com/Beckschen/ViTamin)| ![Pic](https://github.com/HeChengHui/CVPR2024/blob/main/Papers/Topics/Vision%20LLM/assets/WhatsApp%20Image%202024-07-04%20at%2022.49.48.jpeg)
|  [GLaMM: Pixel Grounding Large Multimodal Model ](https://openaccess.thecvf.com/content/CVPR2024/html/Rasheed_GLaMM_Pixel_Grounding_Large_Multimodal_Model_CVPR_2024_paper.html)| | [![GitHub](https://img.shields.io/github/stars/mbzuai-oryx/groundingLMM?style=social)](https://github.com/mbzuai-oryx/groundingLMM)| ![Pic](https://github.com/HeChengHui/CVPR2024/blob/main/Papers/Topics/Vision%20LLM/assets/WhatsApp%20Image%202024-07-04%20at%2023.02.20.jpeg)
| [V?: Guided Visual Search as a Core Mechanism in Multimodal LLMs ](https://openaccess.thecvf.com/content/CVPR2024/html/Wu_V_Guided_Visual_Search_as_a_Core_Mechanism_in_Multimodal_CVPR_2024_paper.html)| ![31596](https://github.com/HeChengHui/CVPR2024/assets/84503515/a0cb1820-172a-4fb9-8dc5-d894aed5af30)| [![GitHub](https://img.shields.io/github/stars/penghao-wu/vstar?style=social)](https://github.com/penghao-wu/vstar)<br> [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=vlsUqJhiCns)

---

### Video-Language
|Title|Poster|Resources|Pic|
|------|------|------|------|
| [Distilling Vision-Language Models on Millions of Videos ](https://openaccess.thecvf.com/content/CVPR2024/html/Zhao_Distilling_Vision-Language_Models_on_Millions_of_Videos_CVPR_2024_paper.html)| ![30028](https://github.com/HeChengHui/CVPR2024/assets/84503515/4f9c0fa3-4468-4e0e-99bb-f4f69eb3db94)| [![Github Pages](https://img.shields.io/badge/github%20pages-121013?style=for-the-badge&logo=github&logoColor=white)](https://zhaoyue-zephyrus.github.io/video-instruction-tuning/)


---

### Misinformation Detection
|Title|Poster|Resources|Pic|
|------|------|------|------|
| [SNIFFER: Multimodal Large Language Model for Explainable Out-of-Context Misinformation Detection ](https://openaccess.thecvf.com/content/CVPR2024/html/Qi_SNIFFER_Multimodal_Large_Language_Model_for_Explainable_Out-of-Context_Misinformation_Detection_CVPR_2024_paper.html)| <img width="1731" alt="31274" src="https://github.com/HeChengHui/CVPR2024/assets/84503515/99b1b5e1-4ae2-4732-a433-9521978078c6"> | [![Github Pages](https://img.shields.io/badge/github%20pages-121013?style=for-the-badge&logo=github&logoColor=white)](https://pengqi.site/Sniffer/) <br> [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=zPTZnz9nhlI)
