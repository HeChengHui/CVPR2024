### Vision LLM
|Title|Poster|Resources|Pic|
|------|------|------|------|
| ‚≠ê[Improved Baselines with Visual Instruction Tuning ](https://openaccess.thecvf.com/content/CVPR2024/html/Liu_Improved_Baselines_with_Visual_Instruction_Tuning_CVPR_2024_paper.html)|LLaVA|[![GitHub](https://img.shields.io/github/stars/haotian-liu/LLaVA?style=social)](https://github.com/haotian-liu/LLaVA)|![Pic](https://github.com/HeChengHui/CVPR2024/blob/main/Papers/Topics/Vision%20LLM/assets/WhatsApp%20Image%202024-07-12%20at%2000.09.17.jpeg)
| ‚≠ê[mPLUG-Owl2: Revolutionizing Multi-modal Large Language Model with Modality Collaboration ](https://openaccess.thecvf.com/content/CVPR2024/html/Ye_mPLUG-Owl2_Revolutionizing_Multi-modal_Large_Language_Model_with_Modality_Collaboration_CVPR_2024_paper.html)| ![31761](https://github.com/HeChengHui/CVPR2024/assets/84503515/acd87e2e-f0e8-46cc-9ddb-a21a981b195e)| [![GitHub](https://img.shields.io/github/stars/X-PLUG/mPLUG-Owl?style=social)](https://github.com/X-PLUG/mPLUG-Owl)
| ‚≠ê[Chat-UniVi: Unified Visual Representation Empowers Large Language Models with Image and Video Understanding ](https://openaccess.thecvf.com/content/CVPR2024/html/Jin_Chat-UniVi_Unified_Visual_Representation_Empowers_Large_Language_Models_with_Image_CVPR_2024_paper.html)| ![31713](https://github.com/HeChengHui/CVPR2024/assets/84503515/ad09e3da-deb9-482b-ba6f-45bd213ce3e4)| [![GitHub](https://img.shields.io/github/stars/PKU-YuanGroup/Chat-UniVi?style=social)](https://github.com/PKU-YuanGroup/Chat-UniVi)<br> [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=qlaFdxcPUIY)
| ‚≠ê[Honeybee: Locality-enhanced Projector for Multimodal LLM ](https://openaccess.thecvf.com/content/CVPR2024/html/Cha_Honeybee_Locality-enhanced_Projector_for_Multimodal_LLM_CVPR_2024_paper.html)|![29641](https://github.com/HeChengHui/CVPR2024/assets/84503515/1378d9f5-3750-4911-8824-ff45adb3e68a) | [![GitHub](https://img.shields.io/github/stars/kakaobrain/honeybee?style=social)](https://github.com/kakaobrain/honeybee)<br> [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=auVcQ4ZhEb0)
| ‚≠ê[Question Aware Vision Transformer for Multimodal Reasoning ](https://openaccess.thecvf.com/content/CVPR2024/html/Ganz_Question_Aware_Vision_Transformer_for_Multimodal_Reasoning_CVPR_2024_paper.html)| ![31218](https://github.com/HeChengHui/CVPR2024/assets/84503515/d7734701-1a24-4fbd-8c71-f33780b1d167)| [![GitHub](https://img.shields.io/github/stars/amazon-science/QA-ViT?style=social)](https://github.com/amazon-science/QA-ViT)<br> [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=OCM-O60wxJg)
| ‚≠ê[Monkey: Image Resolution and Text Label Are Important Things for Large Multi-modal Models ](https://openaccess.thecvf.com/content/CVPR2024/html/Li_Monkey_Image_Resolution_and_Text_Label_Are_Important_Things_for_CVPR_2024_paper.html)| |[![GitHub](https://img.shields.io/github/stars/Yuliang-Liu/Monkey?style=social)](https://github.com/Yuliang-Liu/Monkey)<br> [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=M19IOD6iMXU)| ![Pic](https://github.com/HeChengHui/CVPR2024/blob/main/Papers/Topics/Vision%20LLM/assets/WhatsApp%20Image%202024-07-12%20at%2000.18.05.jpeg)
| [ViTamin: Designing Scalable Vision Models in the Vision-Language Era ](https://openaccess.thecvf.com/content/CVPR2024/html/Chen_ViTamin_Designing_Scalable_Vision_Models_in_the_Vision-Language_Era_CVPR_2024_paper.html)| | [![GitHub](https://img.shields.io/github/stars/Beckschen/ViTamin?style=social)](https://github.com/Beckschen/ViTamin)| ![Pic](https://github.com/HeChengHui/CVPR2024/blob/main/Papers/Topics/Vision%20LLM/assets/WhatsApp%20Image%202024-07-04%20at%2022.49.48.jpeg)
| [VILA: On Pre-training for Visual Language Models ](https://openaccess.thecvf.com/content/CVPR2024/html/Lin_VILA_On_Pre-training_for_Visual_Language_Models_CVPR_2024_paper.html)| ![30868](https://github.com/HeChengHui/CVPR2024/assets/84503515/3f43a54c-ca53-4762-b154-704c85f76bcb)| [![GitHub](https://img.shields.io/github/stars/NVlabs/VILA?style=social)](https://github.com/NVlabs/VILA)<br> [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=0RKw7whiPSE)
| ‚≠ê[Jack of All Tasks Master of Many: Designing General-Purpose Coarse-to-Fine Vision-Language Model ](https://openaccess.thecvf.com/content/CVPR2024/html/Pramanick_Jack_of_All_Tasks_Master_of_Many_Designing_General-Purpose_Coarse-to-Fine_CVPR_2024_paper.html)| |[![Github Pages](https://img.shields.io/badge/github%20pages-121013?style=for-the-badge&logo=github&logoColor=white)](https://shramanpramanick.github.io/VistaLLM/)|![Pic](https://github.com/HeChengHui/CVPR2024/blob/main/Papers/Topics/Vision%20LLM/assets/WhatsApp%20Image%202024-07-05%20at%2014.54.55.jpeg)

---

### Grounding
|Title|Poster|Resources|Pic|
|------|------|------|------|
| [ViP-LLaVA: Making Large Multimodal Models Understand Arbitrary Visual Prompts ](https://openaccess.thecvf.com/content/CVPR2024/html/Cai_ViP-LLaVA_Making_Large_Multimodal_Models_Understand_Arbitrary_Visual_Prompts_CVPR_2024_paper.html)|![29580](https://github.com/HeChengHui/CVPR2024/assets/84503515/47b988d2-546d-451d-b713-2cf8b5b287cf)| [![GitHub](https://img.shields.io/github/stars/WisconsinAIVision/ViP-LLaVA?style=social)](https://github.com/WisconsinAIVision/ViP-LLaVA)<br> [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=j_l1bRQouzc)
|  [GLaMM: Pixel Grounding Large Multimodal Model ](https://openaccess.thecvf.com/content/CVPR2024/html/Rasheed_GLaMM_Pixel_Grounding_Large_Multimodal_Model_CVPR_2024_paper.html)| | [![GitHub](https://img.shields.io/github/stars/mbzuai-oryx/groundingLMM?style=social)](https://github.com/mbzuai-oryx/groundingLMM)| ![Pic](https://github.com/HeChengHui/CVPR2024/blob/main/Papers/Topics/Vision%20LLM/assets/WhatsApp%20Image%202024-07-04%20at%2023.02.20.jpeg)
| [V?: Guided Visual Search as a Core Mechanism in Multimodal LLMs ](https://openaccess.thecvf.com/content/CVPR2024/html/Wu_V_Guided_Visual_Search_as_a_Core_Mechanism_in_Multimodal_CVPR_2024_paper.html)| ![31596](https://github.com/HeChengHui/CVPR2024/assets/84503515/a0cb1820-172a-4fb9-8dc5-d894aed5af30)| [![GitHub](https://img.shields.io/github/stars/penghao-wu/vstar?style=social)](https://github.com/penghao-wu/vstar)<br> [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=vlsUqJhiCns)
| [See Say and Segment: Teaching LMMs to Overcome False Premises ](https://openaccess.thecvf.com/content/CVPR2024/html/Wu_See_Say_and_Segment_Teaching_LMMs_to_Overcome_False_Premises_CVPR_2024_paper.html)| ![31231](https://github.com/HeChengHui/CVPR2024/assets/84503515/c00821b1-e35d-49f1-b53b-bc2c327786aa)| [![GitHub](https://img.shields.io/github/stars/see-say-segment/sesame?style=social)](https://github.com/see-say-segment/sesame)<br> [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=-TXCR-m3MJ4)
| [PixelLM: Pixel Reasoning with Large Multimodal Model ](https://openaccess.thecvf.com/content/CVPR2024/html/Ren_PixelLM_Pixel_Reasoning_with_Large_Multimodal_Model_CVPR_2024_paper.html)| ![31700](https://github.com/HeChengHui/CVPR2024/assets/84503515/d560ec88-aa8a-4191-89b8-fcbe407abf96)| [![GitHub](https://img.shields.io/github/stars/MaverickRen/PixelLM?style=social)](https://github.com/MaverickRen/PixelLM)
| [RegionGPT: Towards Region Understanding Vision Language Model ](https://openaccess.thecvf.com/content/CVPR2024/html/Guo_RegionGPT_Towards_Region_Understanding_Vision_Language_Model_CVPR_2024_paper.html)|![31126](https://github.com/HeChengHui/CVPR2024/assets/84503515/65e0ecbf-32ba-4980-876d-1abde8259be5)| [![Github Pages](https://img.shields.io/badge/github%20pages-121013?style=for-the-badge&logo=github&logoColor=white)](https://guoqiushan.github.io/regiongpt.github.io/) <br> [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=C4IHpFrUb9U)

---

### Agent
|Title|Poster|Resources|Pic|
|------|------|------|------|
| ‚≠ê[CogAgent: A Visual Language Model for GUI Agents ](https://openaccess.thecvf.com/content/CVPR2024/html/Hong_CogAgent_A_Visual_Language_Model_for_GUI_Agents_CVPR_2024_paper.html)| ![30177](https://github.com/HeChengHui/CVPR2024/assets/84503515/614aa358-3742-44e7-8503-a5fc1f3ea96e)| [![GitHub](https://img.shields.io/github/stars/THUDM/CogVLM?style=social)](https://github.com/THUDM/CogVLM)
| [AssistGUI: Task-Oriented PC Graphical User Interface Automation ](https://openaccess.thecvf.com/content/CVPR2024/html/Gao_AssistGUI_Task-Oriented_PC_Graphical_User_Interface_Automation_CVPR_2024_paper.html)|| [![GitHub](https://img.shields.io/github/stars/showlab/assistgui?style=social)](https://github.com/showlab/assistgui)| ![Pic](https://github.com/HeChengHui/CVPR2024/blob/main/Papers/Topics/Vision%20LLM/assets/WhatsApp%20Image%202024-07-05%20at%2000.04.23.jpeg)

---

### Video-Language
|Title|Poster|Resources|Pic|
|------|------|------|------|
|‚≠ê[ VTimeLLM: Empower LLM to Grasp Video Moments ](https://openaccess.thecvf.com/content/CVPR2024/html/Huang_VTimeLLM_Empower_LLM_to_Grasp_Video_Moments_CVPR_2024_paper.html)| ![31139](https://github.com/HeChengHui/CVPR2024/assets/84503515/49d63950-9fb8-41c4-87de-746eaac74a48)| [![GitHub](https://img.shields.io/github/stars/huangb23/VTimeLLM?style=social)](https://github.com/huangb23/VTimeLLM)<br> [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=enbP-Up3Hnw)
| ‚≠ê[MVBench: A Comprehensive Multi-modal Video Understanding Benchmark ](https://openaccess.thecvf.com/content/CVPR2024/html/Li_MVBench_A_Comprehensive_Multi-modal_Video_Understanding_Benchmark_CVPR_2024_paper.html)|![31654](https://github.com/HeChengHui/CVPR2024/assets/84503515/0b3f30ed-cc44-4362-9803-b6a3255bfffb)| [![GitHub](https://img.shields.io/github/stars/OpenGVLab/Ask-Anything?style=social)](https://github.com/OpenGVLab/Ask-Anything/tree/main/video_chat2)<br> [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=OMXlbt7A2OU)
| [MA-LMM: Memory-Augmented Large Multimodal Model for Long-Term Video Understanding ](https://openaccess.thecvf.com/content/CVPR2024/html/He_MA-LMM_Memory-Augmented_Large_Multimodal_Model_for_Long-Term_Video_Understanding_CVPR_2024_paper.html)| ![30043](https://github.com/HeChengHui/CVPR2024/assets/84503515/92ccb2bd-5d83-41af-81e3-4d708bc637e0)| [![GitHub](https://img.shields.io/github/stars/boheumd/MA-LMM?style=social)](https://github.com/boheumd/MA-LMM)<br> [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=wwIGNj98Hc0)
| [TimeChat: A Time-sensitive Multimodal Large Language Model for Long Video Understanding ](https://openaccess.thecvf.com/content/CVPR2024/html/Ren_TimeChat_A_Time-sensitive_Multimodal_Large_Language_Model_for_Long_Video_CVPR_2024_paper.html)| ![30015](https://github.com/HeChengHui/CVPR2024/assets/84503515/e9f2b922-fb83-408b-b9b5-888e929cc653)| [![GitHub](https://img.shields.io/github/stars/RenShuhuai-Andy/TimeChat?style=social)](https://github.com/RenShuhuai-Andy/TimeChat)
| [OmniViD: A Generative Framework for Universal Video Understanding ](https://openaccess.thecvf.com/content/CVPR2024/html/Wang_OmniViD_A_Generative_Framework_for_Universal_Video_Understanding_CVPR_2024_paper.html)| ![31403](https://github.com/HeChengHui/CVPR2024/assets/84503515/3693d936-da69-4ed5-a091-311e58ae76d4)| [![GitHub](https://img.shields.io/github/stars/wdrink/OmniVid?style=social)](https://github.com/wdrink/OmniVid)<br> [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=cdcbkzV4aRg)
| [MovieChat: From Dense Token to Sparse Memory for Long Video Understanding ](https://openaccess.thecvf.com/content/CVPR2024/html/Song_MovieChat_From_Dense_Token_to_Sparse_Memory_for_Long_Video_CVPR_2024_paper.html)| ![29526](https://github.com/HeChengHui/CVPR2024/assets/84503515/669f88e2-b6ce-44e6-9399-da08fba5d1f8)| [![GitHub](https://img.shields.io/github/stars/rese1f/MovieChat?style=social)](https://github.com/rese1f/MovieChat)<br> [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=g985zv12lyg)
| [Distilling Vision-Language Models on Millions of Videos ](https://openaccess.thecvf.com/content/CVPR2024/html/Zhao_Distilling_Vision-Language_Models_on_Millions_of_Videos_CVPR_2024_paper.html)| ![30028](https://github.com/HeChengHui/CVPR2024/assets/84503515/4f9c0fa3-4468-4e0e-99bb-f4f69eb3db94)| [![Github Pages](https://img.shields.io/badge/github%20pages-121013?style=for-the-badge&logo=github&logoColor=white)](https://zhaoyue-zephyrus.github.io/video-instruction-tuning/)

---

### Misinformation Detection
|Title|Poster|Resources|Pic|
|------|------|------|------|
| [SNIFFER: Multimodal Large Language Model for Explainable Out-of-Context Misinformation Detection ](https://openaccess.thecvf.com/content/CVPR2024/html/Qi_SNIFFER_Multimodal_Large_Language_Model_for_Explainable_Out-of-Context_Misinformation_Detection_CVPR_2024_paper.html)| <img width="1731" alt="31274" src="https://github.com/HeChengHui/CVPR2024/assets/84503515/99b1b5e1-4ae2-4732-a433-9521978078c6"> | [![Github Pages](https://img.shields.io/badge/github%20pages-121013?style=for-the-badge&logo=github&logoColor=white)](https://pengqi.site/Sniffer/) <br> [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=zPTZnz9nhlI)

---

### Hallucination Mitigation
|Title|Poster|Resources|Pic|
|------|------|------|------|
| ‚≠ê[OPERA: Alleviating Hallucination in Multi-Modal Large Language Models via Over-Trust Penalty and Retrospection-Allocation ](https://openaccess.thecvf.com/content/CVPR2024/html/Huang_OPERA_Alleviating_Hallucination_in_Multi-Modal_Large_Language_Models_via_Over-Trust_CVPR_2024_paper.html)|![30961](https://github.com/HeChengHui/CVPR2024/assets/84503515/ac08dda2-a0ac-4c58-b7ab-38e6d5437a9a)| [![GitHub](https://img.shields.io/github/stars/shikiw/OPERA?style=social)](https://github.com/shikiw/OPERA)
| ‚≠ê[Mitigating Object Hallucinations in Large Vision-Language Models through Visual Contrastive Decoding ](https://openaccess.thecvf.com/content/CVPR2024/html/Leng_Mitigating_Object_Hallucinations_in_Large_Vision-Language_Models_through_Visual_Contrastive_CVPR_2024_paper.html)||[![GitHub](https://img.shields.io/github/stars/DAMO-NLP-SG/VCD?style=social)](https://github.com/DAMO-NLP-SG/VCD)

---
### Benchmark
|Title|Poster|Resources|Pic|
|------|------|------|------|
| üèÜ‚≠ê[MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI ](https://openaccess.thecvf.com/content/CVPR2024/html/Yue_MMMU_A_Massive_Multi-discipline_Multimodal_Understanding_and_Reasoning_Benchmark_for_CVPR_2024_paper.html)|![31040](https://github.com/HeChengHui/CVPR2024/assets/84503515/9549f29c-5395-4ffd-80ae-8c512b76565e)| [![GitHub](https://img.shields.io/github/stars/MMMU-Benchmark/MMMU?style=social)](https://github.com/MMMU-Benchmark/MMMU)
