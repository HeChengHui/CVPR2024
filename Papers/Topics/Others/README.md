|Title|Poster|Resources|Pic|
|------|------|------|------|
| [From a Bird's Eye View to See: Joint Camera and Subject Registration without the Camera Calibration](https://openaccess.thecvf.com/content/CVPR2024/html/Qian_From_a_Birds_Eye_View_to_See_Joint_Camera_and_CVPR_2024_paper.html) | ![Poster](https://github.com/HeChengHui/CVPR2024/blob/main/Papers/Topics/Others/assets/29340.png) | [![GitHub](https://img.shields.io/github/stars/zekunqian/bevsee?style=social)](https://github.com/zekunqian/bevsee) <br> [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=AQivxJiuwKc)
|  [Day-Night Cross-domain Vehicle Re-identification ](https://openaccess.thecvf.com/content/CVPR2024/html/Li_Day-Night_Cross-domain_Vehicle_Re-identification_CVPR_2024_paper.html)| ![30038](https://github.com/HeChengHui/CVPR2024/assets/84503515/76f8ed9a-a8d5-46c0-a1e7-068a5c607cdc) | [![GitHub](https://img.shields.io/github/stars/chenjingong/DN-ReID?style=social)](https://github.com/chenjingong/DN-ReID)
| üèÜ‚≠ê[EGTR: Extracting Graph from Transformer for Scene Graph Generation ](https://openaccess.thecvf.com/content/CVPR2024/html/Im_EGTR_Extracting_Graph_from_Transformer_for_Scene_Graph_Generation_CVPR_2024_paper.html)|![image](https://github.com/user-attachments/assets/2197e567-6a8c-4ff3-acfa-5ea873613f38)| [![GitHub](https://img.shields.io/github/stars/naver-ai/egtr?style=social)](https://github.com/naver-ai/egtr) <br> [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=w4DWd3mp2Wk)

---

### Explainability (_Do read the full list for findings/toolkits useful for your tasks_)
|Title|Poster|Resources|Findings|
|------|------|------|------|
| ‚≠ê[Visual Concept Connectome (VCC): Open World Concept Discovery and their Interlayer Connections in Deep Models ](https://openaccess.thecvf.com/content/CVPR2024/html/Kowal_Visual_Concept_Connectome_VCC_Open_World_Concept_Discovery_and_their_CVPR_2024_paper.html)| ![Poster](https://github.com/HeChengHui/CVPR2024/blob/main/Papers/Topics/Others/assets/29374.png) | [![GitHub](https://img.shields.io/github/stars/YorkUCVIL/VCC?style=social)](https://github.com/YorkUCVIL/VCC)<br> [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=_PW4MQXRsKM)| <details><summary>Claude-3.5-Sonnet</summary>![image](https://github.com/HeChengHui/CVPR2024/assets/84503515/ff108cd9-5f6c-4093-865b-5c760a500f7f)<br>![image](https://github.com/HeChengHui/CVPR2024/assets/84503515/0c986076-f86b-4c9b-905b-4e54018dd270)</details>
|  üèÜ‚≠ê[Comparing the Decision-Making Mechanisms by Transformers and CNNs via Explanation Methods ](https://openaccess.thecvf.com/content/CVPR2024/html/Jiang_Comparing_the_Decision-Making_Mechanisms_by_Transformers_and_CNNs_via_Explanation_CVPR_2024_paper.html)| ![Poster](https://github.com/HeChengHui/CVPR2024/blob/main/Papers/Topics/Others/assets/30580.png) | [![Github Pages](https://img.shields.io/badge/github%20pages-121013?style=for-the-badge&logo=github&logoColor=white)](https://mingqij.github.io/projects/cdmmtc/) <br> [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=VpqZwFUJMhw)| <details><summary>Claude-3.5-Sonnet</summary>![image](https://github.com/HeChengHui/CVPR2024/assets/84503515/11b9d66c-fd74-436f-b3d8-3ddf025e7a68)</details>
|  ‚≠ê[Understanding Video Transformers via Universal Concept Discovery ](https://openaccess.thecvf.com/content/CVPR2024/html/Kowal_Understanding_Video_Transformers_via_Universal_Concept_Discovery_CVPR_2024_paper.html)| ![31314](https://github.com/HeChengHui/CVPR2024/assets/84503515/b73450af-e9e1-4017-8cbd-a658ce3e335c) | [![GitHub](https://img.shields.io/github/stars/YorkUCVIL/VTCD?style=social)](https://github.com/YorkUCVIL/VTCD)<br> [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=K2RHemQtjzY)| <details><summary>Claude-3.5-Sonnet</summary>![image](https://github.com/HeChengHui/CVPR2024/assets/84503515/7cae0de6-a822-4d27-9cf8-e4ff180421e1)</details>
| ‚≠ê[Brain Decodes Deep Nets ](https://openaccess.thecvf.com/content/CVPR2024/html/Yang_Brain_Decodes_Deep_Nets_CVPR_2024_paper.html)| ![29908](https://github.com/HeChengHui/CVPR2024/assets/84503515/9d9d2f56-2036-43bc-9e2d-50a51e14e5bc)| [![GitHub](https://img.shields.io/github/stars/huzeyann/BrainDecodesDeepNets?style=social)](https://github.com/huzeyann/BrainDecodesDeepNets)<br> [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=Qh49zQQCW1g)
|  [WWW: A Unified Framework for Explaining What Where and Why of Neural Networks by Interpretation of Neuron Concepts ](https://openaccess.thecvf.com/content/CVPR2024/html/Ahn_WWW_A_Unified_Framework_for_Explaining_What_Where_and_Why_CVPR_2024_paper.html)| ![31445](https://github.com/HeChengHui/CVPR2024/assets/84503515/3180f170-1003-47fa-97e2-3134fb5723fd) | [![GitHub](https://img.shields.io/github/stars/ailab-kyunghee/WWW?style=social)](https://github.com/ailab-kyunghee/WWW)<br> [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=DKf15DfeO_o)

---

### Dataset
|Title|Poster|Resources|Findings|
|------|------|------|------|
| ‚≠ê[BEHAVIOR Vision Suite: Customizable Dataset Generation via Simulation ](https://openaccess.thecvf.com/content/CVPR2024/html/Ge_BEHAVIOR_Vision_Suite_Customizable_Dataset_Generation_via_Simulation_CVPR_2024_paper.html)|![29680](https://github.com/HeChengHui/CVPR2024/assets/84503515/7069b8da-5707-496e-a93b-673cf718366d)| [![GitHub](https://img.shields.io/github/stars/behavior-vision-suite/behavior-vision-suite.github.io?style=social)](https://github.com/behavior-vision-suite/behavior-vision-suite.github.io)<br> [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=AYkFkBsxGi8)
| [Domain Gap Embeddings for Generative Dataset Augmentation ](https://openaccess.thecvf.com/content/CVPR2024/html/Wang_Domain_Gap_Embeddings_for_Generative_Dataset_Augmentation_CVPR_2024_paper.html)|![31369](https://github.com/user-attachments/assets/7f24d753-b044-4da1-98bf-07f09050b738)| [![GitHub](https://img.shields.io/github/stars/humansensinglab/DoGE?style=social)](https://github.com/humansensinglab/DoGE)| ![Pic](https://github.com/HeChengHui/CVPR2024/blob/main/Papers/Topics/Others/assets/WhatsApp%20Image%202024-07-13%20at%2000.12.58.jpeg)
