|Title|Poster|Resources|Pic|
|------|------|------|------|
| ‚≠ê[General Object Foundation Model for Images and Videos at Scale ](https://openaccess.thecvf.com/content/CVPR2024/html/Wu_General_Object_Foundation_Model_for_Images_and_Videos_at_Scale_CVPR_2024_paper.html)| ![Poster](https://github.com/HeChengHui/CVPR2024/blob/main/Papers/Topics/Vision%20LLM/assets/29939.png) | [![GitHub](https://img.shields.io/github/stars/FoundationVision/GLEE?style=social)](https://github.com/FoundationVision/GLEE)<br> [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=PSVhfTPx0GQ)
| [Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks ](https://openaccess.thecvf.com/content/CVPR2024/html/Xiao_Florence-2_Advancing_a_Unified_Representation_for_a_Variety_of_Vision_CVPR_2024_paper.html)| ![Poster](https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202024/30529.png?t=1717455193.7819567) | [![HuggingFace](https://img.shields.io/badge/hugging_face-1?style=for-the-badge&logo=huggingface&logoColor=%23FFD21E&color=white)](https://huggingface.co/microsoft/Florence-2-large) <br> [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=cOlyA00K1ec)
